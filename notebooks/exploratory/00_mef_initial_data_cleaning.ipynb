{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "The goal of this notebook is to create a standardized method for processing our data. The original datasets downloaded from the King County website are extremely large (see Data Download section of the readme), but by the end of this notebook they will be much smaller, allowing all further analysis to be performed easily with Pandas. \n",
    "\n",
    "## Process Overview:\n",
    "### 1. Import\n",
    "All data in the project will be stored in a 'data' folder found in the projects main directory.\n",
    "\n",
    "### 2. Filtering\n",
    "Once imported, the data will then be filtered to include only sales and properties that would be of interest to our target audience, which is primarily prospective homeowners.\n",
    "\n",
    "### 3. Export\n",
    "Once properly filter, the new dataframes will be exported with the suffix '_filtered'. For instance EXTR_RPSale.csv will become EXTR_RPSale_filtered.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import\n",
    "\n",
    "### A. Set-up\n",
    "Before we begin the data import, we'll set up our notebook with a few important modules and variables. First we'll add our repository home directory to our path so that we can import our custom modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's add the project directory to our module path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll import the remainder of the modules we'll be using, as well as setting our data_folder variable to the path of our data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also import the rest of our modules\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from src import data_cleaning\n",
    "\n",
    "data_folder = '../../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data Import\n",
    "\n",
    "We'll be looking at only two of our CSVs for the initial data cleaning process. These CSV's are quite large and will take some time to load. \n",
    "\n",
    "#### Note on Unique Identifiers\n",
    "Note the parameters passed to the read_csv method below. The data types of a few of the columns MUST be declared through the dtypes method. This is because the unique identifier for a piece of land will be created using the Major and Minor columns. For example, the first entry in our sales data will have a Major value of 919715 and a Minor value of 0200, so our unique identifer would be 9197150200. These values are numeric, but are also zero padded, therefore importing as an integer would remove the zero padding and result in an incorrect unique identifer of 919715200. We'll go into more detail on the unique identifiers (PINS) later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_sale = pd.read_csv(data_folder+'EXTR_RPSale.csv', dtype={'Major': 'string', 'Minor':'string'})\n",
    "res_bldg = pd.read_csv(data_folder+'EXTR_ResBldg.csv', dtype={'Major': 'string', 'Minor':'string', 'ZipCode': 'string'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filtering\n",
    "\n",
    "The motivation behind filtering the data set comes from our target audience, home buyers. Our original data set contains sales deeds that may not provide accurate information for home buyers, such as foreclosure deeds as well as sales pertaining to commercial or public property.\n",
    "\n",
    "Specifically, we are looking at the following properties:\n",
    "- A. Property that was sold in 2019, to see the factors that affect the current market.\n",
    "\n",
    "- B. Property that was sold, as opposed to foreclosed or transferred as part of a settlement.\n",
    "\n",
    "- C. Property that is residential, as opposed to commercial.\n",
    "\n",
    "- D. Property that was not 'sold' for a value of zero dollars, as these represent sales circumstances such as inheritance, which does not provide accurate portrayal of a property's value.\n",
    "\n",
    "- E. Property that has one building on a given tax parcel. Our data does not include an accurate way to differentiate the values for multiple homes on a single parcel, so these will be excluded to maintain accuracy.\n",
    "\n",
    "- F. Property that is not a mansion, simply because first time homebuyers are likely not investing in  mansions.\n",
    "\n",
    "Each of the previous criteria represent one filter that will be applied to the data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Was sold in 2019\n",
    "\n",
    "This filter cuts the most substantial amount of entries from our data set, which records sales from at least a few decades ago. It's using a filter function that first converts all document dates to a pandas DateTime object, then filters all events that are not from 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_a = data_cleaning.filter_data_by_year(rp_sale, 2019)\n",
    "filter_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It cuts about 2 million entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Change in dataset size: {len(filter_a) - len(rp_sale)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately we'll need normally distributed data in order for our regression model to fit, so let's keep track of what these filters do to our data. Here's a look at what the data looks like right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(filter_a.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll keep track of that as we go throughout our filtering process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Is listed as some sale other than foreclosures, settlements, etc. \n",
    "i.e. a standard sale.\n",
    "\n",
    "A note on Look Up codes. For many of our filters, we will be utilizing the data set's 'LookUp' codes found in the EXTR_LookUp.csv. To use look up codes to find sale reason, we will check the column description in this data sets .doc file (this is found in the projects 'references/data_documentation' folder. Reading the description for the Sale Reason column tells us that it is assigned the lookup code 5. Now we can check the EXTR_LookUp.csv to find the meanings for each associated value found in the sale reason column: \n",
    "\n",
    "- 1 - None \n",
    "- 2 - Assumption         \n",
    "- 3 - Mortgage Assumption \n",
    "- 4 - Foreclosure   \n",
    "- 5 - Trust     \n",
    "- 6 - Executor-to admin guardian \n",
    "- 7 - Testamentary Trust \n",
    "- 8 - Estate Settlement   \n",
    "- 9 - Settlement      \n",
    "- 10 - Property Settlement\n",
    "- 11 - Divorce Settlement      \n",
    "- 12 - Tenancy Partition    \n",
    "- 13 - Community Prop Established    \n",
    "- 14 - Partial Int - love,aff,gft  \n",
    "- 15 - Easement       \n",
    "- 16 - Correction (refiling) \n",
    "- 17 - Trade         \n",
    "- 18 - Other      \n",
    "- 19 - Quit Claim Deed - gift/full or part interest\n",
    "\n",
    "For the purposes of this filter, we want to exclude everything that does not fall under category 1 (None, no reason listed) or 18 (Other, not specified). All other categories are considered non-standard sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_b = filter_a[(filter_a['SaleReason']==1)|(filter_a['SaleReason']==18)]\n",
    "filter_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This removes very little of are data. This is unsurprising because most non-standard sales are listed with a zero sale price and were thus filtered out with the previous non-zero filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Change in dataset size: {len(filter_b) - len(filter_a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(filter_b.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Is residential\n",
    "Because we are targetting prospective homeowners, we don't want to look at commercial buildings. The lookup codes assign residential buildings to the PropertyClass values of 7 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_c = filter_b[(filter_b['PropertyClass'] == 7)|(filter_b['PropertyClass'] == 8)]\n",
    "filter_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a substantial filter, removing approximately 25% of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Change in dataset size: {len(filter_c) - len(filter_b)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(filter_c.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to see a clearer picture of our distribution now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Was not sold for zero dollars\n",
    "Most zero valued sales are the result of some non-standard sale such as inheritance. The reason these sales were not caught by the previous filter b is not known. A further investigation on the nature of these zero_valued homes will be found in a separate notebook in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_d = filter_c[filter_c['SalePrice'] != 0]\n",
    "filter_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our second most substantial filter, removing about a third of our 2019 sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Change in dataset size: {len(filter_d) - len(filter_c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(filter_d.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Property is on a parcel that contains only one building.\n",
    "\n",
    "Because the sales data does not specify which building on a given parcel is being sold, we limited our data set to only include parcels that have one building one them. This process requires joining our sales data to our residential building data. In order to do this, we need to create a PIN column on which we can join these two data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Our Unique Identifier: PINs\n",
    "The process of creating the PIN column simply involves concatenating the Major and Minor columns. This is the reason these columns must always be imported as strings, otherwise the zero-padding will be removed and the PINs may become mismatched.\n",
    "\n",
    "A function found in the projects data_cleaning module creates the PIN columns for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bldg = data_cleaning.add_PIN_column(res_bldg)\n",
    "\n",
    "filter_d = data_cleaning.add_PIN_column(filter_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by number of buildings per parcel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the residential building data set represents a single building, and each building is assigned to a parcel via their PIN. If we simply count the number of rows that are assigned to each parcel, we can filter out any parcel number that contains only one building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_buildings = res_bldg.groupby('PIN').BldgNbr.count()\n",
    "number_of_buildings.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of useable PINs\n",
    "Once I filter out all multi-building parcels, I will create a dataframe containing only the PINs of the building I want to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_building_parcels = pd.DataFrame(number_of_buildings[number_of_buildings==1].index) \n",
    "one_building_parcels = one_building_parcels.set_index('PIN')\n",
    "one_building_parcels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I can perform an inner merge with the sales data to quickly filter out all multi-building parcels from the sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_e = pd.merge(filter_d, one_building_parcels, how='inner', on='PIN')\n",
    "filter_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has a noticable effect on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filter_e) - len(filter_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(filter_e.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Removing mansions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_e = pd.merge(filter_e, res_bldg, how='inner', on='PIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_f = filter_e[filter_e.BldgGrade<12]\n",
    "filter_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(filter_f.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the Sales Data\n",
    "\n",
    "Based on the distribution plot we recieved after the final filter was applied, we are still not operating on a normal distribution. The following steps will be performed here simply as a means of demonstrating the process that will be used with each iteration of our model testing. \n",
    "\n",
    "We will first log-transform the sales data and assign those values to a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_f['LogSalePrice'] = np.log(filter_f.SalePrice)\n",
    "sns.distplot(filter_f.LogSalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot more normal than our right skewed distribution from before, now let's remove the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = np.abs(stats.zscore(filter_f.LogSalePrice))\n",
    "no_outliers = filter_f[z_scores < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(no_outliers.LogSalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much closer to the type of distribution that will lend itself to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final PINs creation\n",
    "\n",
    "I will create a csv containing only the unique pins of the properties that fall under our criteria. The PIN csv will be created from the unique values of the PIN column from my filter_five dataframe. This PINS.csv file can be used to filter any additional datasets we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pins = filter_f.PIN.unique()\n",
    "\n",
    "PINS = pd.DataFrame(unique_pins, columns=['PIN']).set_index('PIN')\n",
    "PINS.to_csv(data_folder+'PINS.csv')\n",
    "PINS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bldg_final = pd.merge(res_bldg, PINS, on='PIN', how='inner')\n",
    "res_bldg_final\n",
    "\n",
    "# We use filter_d because it is the most recent dataframe that doesn't have the res_bldg data merged into it. \n",
    "# We want to keep only the columns from the original sales csv\n",
    "sales_final = pd.merge(filter_d, PINS, on='PIN', how='inner')\n",
    "sales_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bldg_final.to_csv(data_folder+'EXTR_ResBldg_final.csv')\n",
    "\n",
    "sales_final.to_csv(data_folder+'EXTR_RPSale_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure you import the original csv with the right arguments, it may need special encoding, and it may need you to specifiy data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that some csv's need tobe read with an encoding argument set to 'latin-1'\n",
    "parcel = pd.read_csv(data_folder+'EXTR_Parcel.csv', dtype={'Major': 'string', 'Minor':'string'}, encoding='latin-1')\n",
    "accessory = pd.read_csv(data_folder+'EXTR_Accessory_V.csv', dtype={'Major': 'string', 'Minor':'string'}, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, filter the file by our PINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel = data_cleaning.add_PIN_column(parcel)\n",
    "accessory = data_cleaning.add_PIN_column(accessory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure it looks okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessory.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter with the list of PINs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINS = pd.read_csv(data_folder+'PINS.csv', dtype={'PIN': 'string'})\n",
    "PINS = PINS.set_index('PIN')\n",
    "\n",
    "\n",
    "parcel_final = parcel.join(PINS, how='inner', on='PIN')\n",
    "print('finished parcels')\n",
    "\n",
    "accessory_final = accessory.join(PINS, how='inner', on='PIN')\n",
    "print('finished accessory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export it the data file, with the suffix '_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_final.to_csv(data_folder+'EXTR_Parcel_final.csv')\n",
    "accessory_final.to_csv(data_folder+'EXTR_Accessory_V_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
